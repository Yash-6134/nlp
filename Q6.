from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec
import pandas as pd, nltk, re

nltk.download('punkt')

# Load dataset
df = pd.read_csv('data.csv')
cols = ['Make','Model','Market Category','Vehicle Size','Vehicle Style',
        'Transmission Type','Driven_Wheels','Engine Fuel Type']
texts = df[cols].fillna('').astype(str).agg(' '.join, axis=1).str.lower()

# Clean only unnecessary chars (keep digits/hyphens)
texts = texts.apply(lambda t: re.sub(r'[^a-z0-9\- ]', '', t))

# --- BoW ---
cv = CountVectorizer()
bow = pd.DataFrame(cv.fit_transform(texts).toarray(), columns=cv.get_feature_names_out())
bow_norm = bow.div(bow.sum(axis=1).replace(0,1), axis=0)

# --- TF-IDF ---
tfidf = TfidfVectorizer()
tfidf_df = pd.DataFrame(tfidf.fit_transform(texts).toarray(), columns=tfidf.get_feature_names_out())

# --- Word2Vec ---
sentences = [nltk.word_tokenize(t) for t in texts]
w2v = Word2Vec(sentences, vector_size=50, window=4, min_count=2, workers=4, epochs=10, seed=42)

print("Most similar to 'suv':", w2v.wv.most_similar('suv', topn=5))
